<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://mint.univ-smb.fr/feed.xml" rel="self" type="application/atom+xml" /><link href="https://mint.univ-smb.fr/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-10-11T15:19:21+00:00</updated><id>https://mint.univ-smb.fr/feed.xml</id><title type="html">ANR-NSF MINT</title><subtitle>Modeling Modern Network Traffic: From Data Representation to Automated Machine Learning
</subtitle><entry><title type="html">NetDiffusion will appear at ACM Sigmetrics 2024</title><link href="https://mint.univ-smb.fr/news/2024/netdiff/" rel="alternate" type="text/html" title="NetDiffusion will appear at ACM Sigmetrics 2024" /><published>2024-02-07T13:00:00+00:00</published><updated>2024-02-07T13:00:00+00:00</updated><id>https://mint.univ-smb.fr/news/2024/netdiff</id><content type="html" xml:base="https://mint.univ-smb.fr/news/2024/netdiff/"><![CDATA[<h1 id="netdiffusion-network-data-augmentation-through-protocol-constrained-traffic-generation">NetDiffusion: Network Data Augmentation Through Protocol-Constrained Traffic Generation</h1>

<p><em>Abstract.</em> Datasets of labeled network traces are essential for a multitude of machine
	learning (ML) tasks in networking, yet their availability is hindered by
	privacy and maintenance concerns, such as data staleness. To
	overcome this limitation, synthetic network traces can often augment
	existing datasets. Unfortunately, current synthetic trace generation methods,
	which typically produce only aggregated flow statistics or a few selected packet
	attributes, do not always suffice, especially when model training relies
    on having features that are only available from packet traces. This shortfall
	manifests in both insufficient statistical resemblance to real traces and
	suboptimal performance on ML tasks when employed for data augmentation.
	In this paper, we apply
	diffusion models to generate high-resolution 
	synthetic network traffic traces. We present <em>NetDiffusion</em>,
	a tool that uses a finely-tuned, controlled variant of a Stable Diffusion
	model to generate synthetic network traffic that is high fidelity and
    conforms to protocol specifications.
	Our evaluation demonstrates that 
	packet captures generated from NetDiffusion can achieve higher statistical similarity to real
	data and improved ML model performance than current
    state-of-the-art approaches (e.g., GAN-based approaches). Furthermore,
	our synthetic traces are compatible with
	common network analysis tools
	and support a myriad of network tasks,
	suggesting that NetDiffusion can serve a broader spectrum of network analysis and testing tasks, extending beyond ML-centric applications.</p>

<h2 id="resources">Resources</h2>
<p>Source code and results: <a href="https://github.com/noise-lab/NetDiffusion_Generator">https://github.com/noise-lab/NetDiffusion_Generator</a></p>

<h3 id="citation-bibtex">Citation bibtex</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{xi2024netdiffusion,
  title={NetDiffusion: Network Data Augmentation Through Protocol-Constrained Traffic Generation},
  author={Jiang, Xi and Liu, Shinan and Gember-Jacobson, Aaron and Nitin Bhagoji, Arjun  and Schmitt, Paul and Bronzino, Francesco and Feamster, Nick},
  journal={Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  year={2024}
}
</code></pre></div></div>]]></content><author><name></name></author><category term="publication," /><category term="software-release" /><summary type="html"><![CDATA[NetDiffusion: Network Data Augmentation Through Protocol-Constrained Traffic Generation]]></summary></entry><entry><title type="html">AMIR will appear at ACM Ubicomp 2023</title><link href="https://mint.univ-smb.fr/news/2023/amir/" rel="alternate" type="text/html" title="AMIR will appear at ACM Ubicomp 2023" /><published>2023-04-03T13:00:00+00:00</published><updated>2023-04-03T13:00:00+00:00</updated><id>https://mint.univ-smb.fr/news/2023/amir</id><content type="html" xml:base="https://mint.univ-smb.fr/news/2023/amir/"><![CDATA[<h1 id="amir-active-multimodal-interaction-recognition-from-video-and-network-traffic-in-connected-environments">AMIR: Active Multimodal Interaction Recognition from Video and Network Traffic in Connected Environments</h1>

<p><em>Abstract.</em> Activity recognition using video data is widely adopted for elder care, monitoring for safety and security, and home automation. Unfortunately, using video data as the basis for activity recognition can be brittle, since models trained on video are often not robust to certain environmental changes, such as camera angle and lighting changes. There has been a proliferation of network-connected devices in home environments. Interactions with these smart devices are associated with network activity, making network data a potential source for recognizing these device interactions. This paper advocates for the synthesis of video and network data for robust interaction recognition in connected environments. We consider machine learning-based approaches for activity recognition, where each labeled activity is associated with both a video capture and an accompanying network traffic trace. We develop a simple but effective framework AMIR (Active Multimodal Interaction Recognition)1 that trains independent models for video and network activity recognition respectively, and subsequently combines the predictions from these models using a meta-learning framework. Whether in lab or at home, this approach reduces the amount of “paired” demonstrations needed to perform accurate activity recognition, where both network and video data are collected simultaneously. Specifically, the method we have developed requires up to 70.83% fewer samples to achieve 85% F1 score than random data collection, and improves accuracy by 17.76% given the same number of samples.</p>

<h2 id="resources">Resources</h2>
<p>You can access the source code of the project as well as detailed documentation at <a href="https://amir-vidnet.github.io">https://amir-vidnet.github.io</a></p>

<h3 id="citation-bibtex">Citation bibtex</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{liu2023amir,
  title={AMIR: Active Multimodal Interaction Recognition from Video and Network Traffic in Connected Environments},
  author={Liu, Shinan and Mangla, Tarun and Shaowang, Ted and Zhao, Jinjin and Paparrizos, John and Krishnan, Sanjay and Feamster, Nick},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={7},
  number={1},
  pages={1--26},
  year={2023},
  publisher={ACM New York, NY, USA}
}
</code></pre></div></div>]]></content><author><name></name></author><category term="publication," /><category term="software-release" /><summary type="html"><![CDATA[AMIR: Active Multimodal Interaction Recognition from Video and Network Traffic in Connected Environments]]></summary></entry><entry><title type="html">LEAF will appear at ACM CoNEXT 2023</title><link href="https://mint.univ-smb.fr/news/2023/leaf/" rel="alternate" type="text/html" title="LEAF will appear at ACM CoNEXT 2023" /><published>2023-04-03T13:00:00+00:00</published><updated>2023-04-03T13:00:00+00:00</updated><id>https://mint.univ-smb.fr/news/2023/leaf</id><content type="html" xml:base="https://mint.univ-smb.fr/news/2023/leaf/"><![CDATA[<h1 id="leaf-navigating-concept-drift-in-cellular-networks">LEAF: Navigating Concept Drift in Cellular Networks</h1>

<p><em>Abstract.</em> Operational networks commonly rely on machine learning models for many tasks, including detecting anomalies, inferring application performance, and forecasting demand. Yet, model accuracy can degrade due to concept drift, where either the relationships between features and the target to be predicted, or the features themselves change. Mitigating concept drift is an essential part of operationalizing machine learning models in general, but is of particular importance in networking’s highly dynamic deployment environments. In this paper, we first characterize concept drift in a large cellular network for a major metropolitan area in the United States. We find that concept drift occurs across many important key performance indicators (KPIs), independently of the model, training set size, and time interval—thus necessitating practical approaches to detect, explain, and mitigate it. We then show that frequent model retraining with newly available data is not sufficient to mitigate concept drift, and can even degrade model accuracy further. Finally, we develop a new methodology for concept drift mitigation, Local Error Approximation of Features (LEAF). LEAF works by detecting drift; explaining the features and time intervals that contribute the most to drift; and mitigating it using forgetting and over-sampling. We evaluate LEAF against industry-standard mitigation approaches (notably, periodic retraining) with more than four years of cellular KPI data. Our tests with a major cellular provider in the US show that LEAF consistently outperforms periodic and triggered re-training on complex, real-world data while reducing costly retraining operations.</p>

<h2 id="resources">Resources</h2>
<p>You will soon been able to access the released dataset accompanying the paper at this page.</p>

<h3 id="citation-bibtex">Citation bibtex</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{liu2023leaf,
  title={LEAF: Navigating Concept Drift in Cellular Networks},
  author={Liu, Shinan and Bronzino, Francesco and Schmitt, Paul and Nitin Bhagoji, Arjun and Feamster, Nick and Crespo, Hector Garcia and Coyle, Timothy and Ward, Brian},
  journal={Proceedings of the ACM on Networking},
  year={2023}
}
</code></pre></div></div>]]></content><author><name></name></author><category term="publication," /><category term="software-release" /><summary type="html"><![CDATA[LEAF: Navigating Concept Drift in Cellular Networks]]></summary></entry><entry><title type="html">Retina will appear at ACM SIGCOMM 2022</title><link href="https://mint.univ-smb.fr/news/2022/retina/" rel="alternate" type="text/html" title="Retina will appear at ACM SIGCOMM 2022" /><published>2022-08-22T13:00:00+00:00</published><updated>2022-08-22T13:00:00+00:00</updated><id>https://mint.univ-smb.fr/news/2022/retina</id><content type="html" xml:base="https://mint.univ-smb.fr/news/2022/retina/"><![CDATA[<h1 id="retina-analyzing-100-gbe-traffic-on-commodity-hardware">Retina: Analyzing 100 GbE Traffic on Commodity Hardware.</h1>

<p><em>Abstract.</em> As network speeds have increased to over 100 Gbps, operators and researchers have lost the ability to easily ask complex questions of reassembled and parsed network traffic. In this paper, we introduce Retina, a software framework that lets users analyze over 100 Gbps of real-world traffic on a single server with no specialized hardware. Retina supports running arbitrary user-defined analysis functions on a wide variety of extensible data representations ranging from raw packets to parsed application-layer handshakes. We introduce a novel filtering mechanism and subscription interface to safely and efficiently process high-speed traffic. Under the hood, Retina implements an efficient data pipeline that strategically discards unneeded traffic and defers expensive processing operations to pre- serve computation for complex analyses. We present the framework architecture, evaluate its performance on production traffic, and explore several applications. Our experiments show that Retina is capable of running sophisticated analyses at over 100 Gbps on a single commodity server and can support 5–100× higher traffic rates than existing solutions, dramatically reducing the effort to complete investigations on real-world networks.</p>

<h2 id="resources">Resources</h2>
<p>You can access the source code of the project as well as detailed documentation at <a href="https://stanford-esrg.github.io/retina/retina_core/">https://stanford-esrg.github.io/retina/retina_core/</a></p>

<h3 id="citation-bibtex">Citation bibtex</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{wan2022retina,
  title={Retina: analyzing 100GbE traffic on commodity hardware},
  author={Wan, Gerry and Gong, Fengchen and Barbette, Tom and Durumeric, Zakir},
  booktitle={Proceedings of the ACM SIGCOMM 2022 Conference},
  pages={530--544},
  year={2022}
}
</code></pre></div></div>]]></content><author><name></name></author><category term="publication," /><category term="software-release" /><summary type="html"><![CDATA[Retina: Analyzing 100 GbE Traffic on Commodity Hardware]]></summary></entry><entry><title type="html">Traffic Refinery will appear at ACM SIGMETRICS 2022</title><link href="https://mint.univ-smb.fr/news/2022/traffic-refinery/" rel="alternate" type="text/html" title="Traffic Refinery will appear at ACM SIGMETRICS 2022" /><published>2022-01-24T13:00:00+00:00</published><updated>2022-01-24T13:00:00+00:00</updated><id>https://mint.univ-smb.fr/news/2022/traffic%20refinery</id><content type="html" xml:base="https://mint.univ-smb.fr/news/2022/traffic-refinery/"><![CDATA[<h1 id="traffic-refinery-cost-aware-network-traffic-analysis">Traffic Refinery: Cost-aware Network Traffic Analysis.</h1>

<p>Relationships between systems costs and model performance would ideally inform
machine learning pipelines during design; yet, most existing network traffic
representation decisions are made <em>a priori</em>, without concern for future use by
models. To enable this exploration, we have created <code class="language-plaintext highlighter-rouge">Traffic Refinery</code>, a system
designed to offer <strong>flexibly extensible network data representations</strong>, the
ability to assess the <strong>systems-related costs</strong> of these representations, and
the <strong>effects of different representations on model performance</strong>.</p>

<h2 id="system-overview">System Overview</h2>
<p><img src="/assets/img/system.png" alt="Traffic Refinery System Overview Diagram" align="center" height="65%" width="65%" /></p>

<p>The figure shows an overview of the system architecture. <code class="language-plaintext highlighter-rouge">Traffic Refinery</code> is
implemented in Go to exploit performance and flexibility, as well as its
built-in benchmarking tools. The system has three components:</p>

<ol>
  <li>A traffic categorization module responsible for associating network traffic
with applications</li>
  <li>A packet capture and processing module that collects network flow statistics
and tracks their state; moreover, this block implements a cache used to store
flow state information</li>
  <li>An aggregation and storage module that queries the flow cache to obtain
features and statistics about each traffic flow and stores higher-level
features concerning the applications of interest for later processing</li>
</ol>

<h2 id="tldr-what-can-you-do-with-traffic-refinery">tl;dr: What Can You Do with Traffic Refinery?</h2>
<ul>
  <li>Traffic (i.e., flows) are classified as “services” using either DNS domains
or IP prefixes that the user can provide. <em>Note: DNS is increasingly
encrypted, making this method less reliable. An area of ongoing research is
privacy-preserving flow categorization.</em></li>
  <li>For each service, users can select from a set of existing features or create
additional ones to collect along with their frequency.</li>
  <li>The system-related costs of each feature can be profiled, enabling users to
explore tradeoffs between ML model performance and feature costs in their
particular environment.</li>
</ul>

<h2 id="why-is-traffic-refinery-necessary">Why is Traffic Refinery Necessary?</h2>
<p>Network management increasingly relies on machine learning to make predictions
about performance and security from network traffic. Often, the representation
of the traffic is as important as the choice of the model. The features that the
model relies on, and the representation of those features, ultimately determine
model accuracy, as well as where and whether the model can be deployed in
practice. Thus, the design and evaluation of these models ultimately requires
understanding not only model accuracy but also the systems costs associated with
deploying the model in an operational network.</p>

<p>To highlight the need for <code class="language-plaintext highlighter-rouge">Traffic Refinery</code>, we show results from our <a href="https://dl.acm.org/doi/10.1145/3366704">prior
work</a> by training multiple ML models to
infer the resolution of encrypted video streaming applications over time using
different data representations: 1) using only L3 features, as would be available
using <code class="language-plaintext highlighter-rouge">netflow</code>; 2) adding transport layer features; and 3) adding application
layer features to L3; and combining all features. The figure below shows the
precision and recall achieved by each representation.</p>

<p><img src="/assets/img/resolution_features.png" alt="Resolution inference features" align="center" height="40%" width="40%" /></p>

<p>As one might expect, a model trained solely with L3 features achieves the
poorest performance. Hence, relying solely on features offered by existing
network infrastructure would produce the worst performing models. On the other
hand, combining Network and Application features results in more than a 10%
increase in both precision and recall. This example showcases how limiting
available data representations to the ones typically available from existing
systems (e.g., NetFlow) can inhibit potential gains, highlighted by the
blue-shaded area.</p>

<p>Of course, any representation is possible if packet traces are the starting
point, but raw packet capture can be prohibitive in operational networks,
especially at high speeds.  The figure below shows the amount of storage
required to collect a one-hour packet capture from a live 10 Gbps link.</p>

<p><img src="/assets/img/storage_profile.png" alt="Storage profile" align="center" height="40%" width="40%" /></p>

<p><code class="language-plaintext highlighter-rouge">Traffic Refinery</code> provides a new framework and system that enables a joint
evaluation of both the conventional notions of machine learning performance
(e.g., model accuracy) and the systems-level costs of different representations
of network traffic.</p>

<h2 id="resources">Resources</h2>
<p>The research paper behind <code class="language-plaintext highlighter-rouge">Traffic Refinery</code> was accepted to SIGMETRICS 2022,
and published in ACM POMACS in December 2021.</p>

<p>You can access the source code of the project as well as detailed documentation at <a href="https://traffic-refinery.github.io">https://traffic-refinery.github.io</a></p>

<h3 id="citation-bibtex">Citation bibtex</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{10.1145/3491052,
    author = {Bronzino, Francesco and Schmitt, Paul and Ayoubi, Sara and Kim, Hyojoon and Teixeira, Renata and Feamster, Nick},
    title = {Traffic Refinery: Cost-Aware Data Representation for Machine Learning on Network Traffic},
    year = {2021},
    issue_date = {December 2021},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {5},
    number = {3},
    url = {https://doi.org/10.1145/3491052},
    doi = {10.1145/3491052},
    journal = {Proc. ACM Meas. Anal. Comput. Syst.},
    month = {dec},
    articleno = {40},
    numpages = {24}
}
</code></pre></div></div>]]></content><author><name></name></author><category term="publication," /><category term="software-release" /><summary type="html"><![CDATA[A cost-aware data representation analysis system for machine learning on network traffic]]></summary></entry><entry><title type="html">The MINT project has started!</title><link href="https://mint.univ-smb.fr/news/2021/kickstart/" rel="alternate" type="text/html" title="The MINT project has started!" /><published>2021-10-21T10:00:00+00:00</published><updated>2021-10-21T10:00:00+00:00</updated><id>https://mint.univ-smb.fr/news/2021/kickstart</id><content type="html" xml:base="https://mint.univ-smb.fr/news/2021/kickstart/"><![CDATA[<p>Both network operations and research depend on the ability to answer questions about network traffic. Decades ago, the questions were simpler: they involved traffic volumes and simple performance metrics. The answers were also more apparent: most traffic was not encrypted, and the answers to most questions were readily apparent from protocol headers and unencrypted packet payloads. Today, operators and researchers are asking more sophisticated questions about application performance, quality of experience (QoE), and malicious traffic originating from IoT devices, as well as trying to predict the impact of potential changes. And yet, as questions are becoming increasingly complex and important, network data is becoming more difficult to obtain. Increased traffic requires operators to make hard decisions about sampling and altogether precludes analyzing individual packets and reassembled streams. Furthermore, traffic is increasingly opaque. Web content has become ubiquitously encrypted, preventing operators from directly inspecting video streams to troubleshoot performance problems. Major services have moved to a handful of IP addresses on large cloud providers like Amazon, Google, and Cloudflare, removing the identity once provided by IP addresses. Networks contain increasingly heterogeneous manufacturer-controlled devices that cannot be troubleshooted locally. As a result, even seemingly simple, but important questions like <em>What content is sent in cleartext?</em> or <em>What is the packet loss for Netflix traffic on my network?</em> are impossible to answer today.</p>

<p>Despite traffic becoming more opaque, it is possible to infer many of the characteristics of traffic most important to operators through statistical learning. Consider for example, monitoring video steaming quality. This has become increasingly difficult as the recent adoption of HTTPS and QUIC prevents directly observing video quality metrics. Our recent work shows that it is possible to infer startup delay and resolution of encrypted video to Netflix and Youtube in real-world homes by training a model on traffic features. And yet, this model and much other previous work on applying machine learning to network operations and security has not made the transition to practice at ISPs. Most models do not perform outside of the isolated laboratory environment in which they were trained and even when robust, they require access to data that cannot be collected and analyzed in real-time on high-speed networks. Building and operationalizing inference models is more than a <em>“simple matter of engineering”</em>. Some of these challenges include: (1) evaluating models at high speeds, including performing flow reassembly at high speeds; (2) coping with dirty data, such as network traces that include other network traffic or training data that is unlabeled or (worse) erroneously labeled; (3) representing network traffic data in formats that are amenable to training; and (4) determining when to retrain these models, due to drift in network traffic patterns over time.</p>

<p>This project aims to make it easier for operators and researchers to ask questions about network traffic. Doing so involves solving new, challenging research questions to create the requisite analytical building blocks required to model traffic on modern networks. Once we have the analysis platform and models in place, we can then turn to helping operators answer questions that help them more effectively run their networks and enabling researchers answer questions that drive discovery. The project involves the following activities:</p>

<ul>
  <li>
    <p>Methods for data representation for network traffic: We will study how to represent traffic data in ways that are amenable to modeling, and that could optimize models for both supervised and unsupervised modeling tasks. This study will explore the impact of representations across four dimensions: (1) timeseries representations; (2) representations across flows; (3) representations at higher layers; and, (4) operations on compressed data.</p>
  </li>
  <li>
    <p>Methods for model selection and benchmarking: We will build on our work on traffic data representation, to develop a set of tools to automatically explore model and traffic representations tailored for network traffic problems. This methods will enable the identification of optimal operation points for models applied to a variety of problems across network management. To support this goal, we will build a large-scale repository of labeled flows across a number of different applications and services as well as evaluate data representations that will be used to build statistical learning models about network traffic.</p>
  </li>
  <li>
    <p>Methods for deploying models in operational networks: we will use the software platforms and algorithmic primitives we built to design new techniques and tools for operators to solve the challenges that block them from transferring developed models from isolated laboratory experiments to real-world deployments. We will support their need to monitor their networks and investigate problems in real time by: (1) extending automated model selection to account for systems costs and real world limitations; (2) addressing the need to be able to determine when models become inaccurate and distinguishing model inaccuracies from problems that are inherent to the network; (3) improve models robustness by investigating a generalized approach for model transfer.</p>
  </li>
</ul>]]></content><author><name></name></author><category term="project-update" /><summary type="html"><![CDATA[Read a quick project description]]></summary></entry></feed>